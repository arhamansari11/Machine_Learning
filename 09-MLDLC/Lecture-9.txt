Machine Learning Devlopement Life Cycle (MLDLC)

SDLC stands for Software Developement Life Cycle.

If you made a software from starting to end you have to follow this SDLC.

Like how to make software , how to deploy , how to test and everything is in SDLC.

MLDLC is the set of guideline that we have to follow when we are working on any ML Base Project.

In this you will find everything that are required from start of the project to the last. Like it give us complete process.

There are 9 steps of Machine Learning :



i ->  Framing the Problem.

Firstly we have to analyze the problem. 

->  Like what's the problem? 

->  How I solve it. 

->  How many peoples are needed for that.

->  How many time needed for that

->  May I used Supervized Learning or UnSupervized Learning.

->  How much data I have. and also many more








ii ->   Gathering the Data.

Then we have to gather the data from various Resources. Following are some resources where from we gather the data.

->  From API ,    CSV Files ,    Web Scraping ,     From Clusters  ( Usually used for big Data )

->  DataBase (We cannot access the data directly from database becuase error for this we use DV [Data Wear House] ) and also we use ETL (Extract Transform Load.) 




iii ->  Data Prepoccessing.

In simple words Data Preprocessing means we clean the data like there are many errors in our data so firsly we have to clean the data.

There are many errors in our data like:

->  Missing Values in our Data

->  Duplicates Values in our Data

->  Outliers (Like our data are coming from various resources it also cause effects)

->  Scale Values (We have to scale all the values equally like 1 cr or decimal form etc.). Its also called Standarization.






iv ->  EDA ( Exploratory Data Analysis )  

So from the word Data Analysis its clear that like we have to analyze the input and output relationship.

Like in EDA we have to make so many experiments to know what are in the data like we discover the hidden patterns , relationships and many more.

In this we make :

-> Visulization ( By Plotting Graphs ) 

-> UniVariant ( Like we analysis independly on every columns like what is the mean , mode and what is the curve.) 

-> ByVariant ( In this we analyze two columns at the same time to get the data )

-> MultiVariant ( In this we analyze the multiple columns at the same time to get the data. ) 

So in the whole step we find the core concepts from the data for futher use.





v ->  Feature Engineering and Selection.

Inputs are called features so Features are very important because our outputs are depends on the inputs. 

So whats the idea of the feature engineering in this we create some new columns by our own.

For example : We have the data of house and we want to sale the house we have 3 columns :- No of washrooms , No of Bedrooms and location.  

So as we want to sell our house we need only square foots area so we make a new column name Area and there we store square feet of the house and remove other columns.

So there is something called Feature Selection.

Like sometimes we have so many features like 100 200. So at there we not move ahead from these 100 , 200 features.

Becuase first of all all these features never helpful in generating our output. So we have to find those features that not impact our input and remove them.

And the second reason are like 100 200 features take a lot of time. So we need to remove them.








vi ->  Model Training , Evaluation and Selection.

So once you are sure about your data like you gather the data , Data Preprocssing and also you do feature engineering and selection on your data.

Then you are almost ready to train your data. Then you gather differents algorithms and  you give the data to these algorithms and train the algorithms on your data.

In this like we check the results of different algorithms and then we decide which algorithm is best for our data.


->  Evaluation.

We evaluate all the models. Like 

->  Model Selection.

In model Selection we get 3 or 4 algorithms then we tune their parameters .

What is Parameter?

In simple words parameters are called settings. Like we tune our tv channels according to our need like films first , then news channels etc.







vii ->  Model Deployement 

So now your model is ready to predict . But our work is not over yet we have to deploy it.

So our model may be desktop app, web app or may be mobile app.

So from this model we make a file called binary file like sort a text file. We use different tools to convert that.

Then we convert this file into an API.



viii -> Testing

We test our model with some trusted customers. If our model works ok we show the changings to all the customer but if it not then we change in the model.

Error may be in data preproccesing , Feature Selection and also many more.




ix -> Optimize

At there we launch our model for all the customers.

After that we perfrom some important steps like:

->  Data Backup

->  Model Backup

->  If our model not works properly what we do all these are covered in that.

















































